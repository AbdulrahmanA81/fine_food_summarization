{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load in trained model"
      ],
      "metadata": {
        "id": "t2dIcvId6OQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "model_to_load_in = \"t5_100k_trained\"\n",
        "load_path = \"/content/drive/MyDrive/CSC413_Final_Project/\" + model_to_load_in + \".pth\"\n",
        "\n",
        "model.load_state_dict(torch.load(load_path, map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ddWhhFKutWl",
        "outputId": "97d4c61b-0cb9-4d7e-afe4-15b3723ce272"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run both models on test set"
      ],
      "metadata": {
        "id": "5bTTJMeZ732n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5TokenizerFast\n",
        "\n",
        "# Convert DataFrame columns to lists\n",
        "reviews = df_val['review'].tolist()\n",
        "summaries = df_val['summary'].tolist()\n",
        "\n",
        "# Tokenize the reviews and summaries\n",
        "tokenized_reviews = tokenizer(reviews, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_REVIEW_LENGTH)\n",
        "tokenized_summaries = tokenizer(summaries, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_REVIEW_LENGTH)\n",
        "\n",
        "# Extract the tensors\n",
        "review_tensors = tokenized_reviews['input_ids']\n",
        "summary_tensors = tokenized_summaries['input_ids']\n",
        "\n",
        "review_tensors = review_tensors.to(device)\n",
        "summary_tensors = summary_tensors.to(device)"
      ],
      "metadata": {
        "id": "mbJBRanSDQ8i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_generated_summaries = []\n",
        "baseline_generated_summaries = []\n",
        "target_summaries = []\n",
        "input_reviews = []\n",
        "\n",
        "def process_batch(reviews, summaries, model, baseline_model):\n",
        "    # Process with the trained model\n",
        "    trained_outputs = model.generate(input_ids=reviews, attention_mask=reviews.ne(tokenizer.pad_token_id), num_beams=4, do_sample=True, min_length=1, max_length=10)\n",
        "    trained_decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in trained_outputs]\n",
        "    trained_predicted_summaries = [nltk.sent_tokenize(decoded.strip())[0] for decoded in trained_decoded_outputs]\n",
        "\n",
        "    # Process with the baseline model\n",
        "    baseline_outputs = baseline_model.generate(input_ids=reviews, attention_mask=reviews.ne(tokenizer.pad_token_id), num_beams=4, do_sample=True, min_length=1, max_length=10)\n",
        "    baseline_decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in baseline_outputs]\n",
        "    baseline_predicted_summaries = [nltk.sent_tokenize(decoded.strip())[0] for decoded in baseline_decoded_outputs]\n",
        "\n",
        "    actual_summaries = [tokenizer.decode(summary, skip_special_tokens=True) for summary in summaries]\n",
        "\n",
        "    return trained_predicted_summaries, baseline_predicted_summaries, actual_summaries\n",
        "\n",
        "# Set a batch size\n",
        "batch_size = 128\n",
        "originalModel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "model.to(device)\n",
        "originalModel.to(device)\n",
        "# Process in batches\n",
        "for i in range(0, len(review_tensors), batch_size):\n",
        "    review_batch = review_tensors[i:i+batch_size]\n",
        "    summary_batch = summary_tensors[i:i+batch_size]\n",
        "\n",
        "    review_batch = review_batch.to(device)\n",
        "    summary_batch = summary_batch.to(device)\n",
        "\n",
        "\n",
        "    trained_preds, baseline_preds, actuals = process_batch(review_batch, summary_batch, model, originalModel)\n",
        "\n",
        "    trained_generated_summaries.extend(trained_preds)\n",
        "    baseline_generated_summaries.extend(baseline_preds)\n",
        "    target_summaries.extend(actuals)\n",
        "    input_reviews.extend([tokenizer.decode(review, skip_special_tokens=True) for review in review_batch])\n",
        "\n",
        "# At this point:\n",
        "# - trained_generated_summaries contains summaries generated by model\n",
        "# - baseline_generated_summaries contains summaries generated by the baseline model\n",
        "# - target_summaries contains the actual summaries\n",
        "# - input_reviews contains the input reviews\n",
        "\n"
      ],
      "metadata": {
        "id": "3LsZ3SQh38Xe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "trained_generated_summaries_trimmed = []\n",
        "baseline_generated_summaries_trimmed = []\n",
        "target_summaries_trimmed = []\n",
        "input_reviews_trimmed = []\n",
        "indices = []\n",
        "\n",
        "for i in range(len(trained_generated_summaries)):\n",
        "  if trained_generated_summaries[i] != baseline_generated_summaries[i]:\n",
        "    trained_generated_summaries_trimmed.append(trained_generated_summaries[i])\n",
        "    baseline_generated_summaries_trimmed.append(baseline_generated_summaries[i])\n",
        "    target_summaries_trimmed.append(target_summaries[i])\n",
        "    input_reviews\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg6vuHJHJrhl",
        "outputId": "88aa0b17-6d6d-4c78-97d9-1eb07dc6ca6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save to CSV"
      ],
      "metadata": {
        "id": "IQ2STUkb8EQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame from the arrays\n",
        "df = pd.DataFrame({\n",
        "    'Trained Model Summaries': trained_generated_summaries_trimmed,\n",
        "    'Baseline Model Summaries': baseline_generated_summaries_trimmed,\n",
        "    'Target Summaries': target_summaries_trimmed,\n",
        "    'Input Reviews': input_reviews_trimmed\n",
        "})\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "csv_file = '/content/summaries_comparison.csv'\n",
        "df.to_csv(csv_file, index=False)\n",
        "\n",
        "# Download the file (uncomment the next line to enable download in Google Colab)\n",
        "from google.colab import files\n",
        "files.download(csv_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "N_0avMrDKQ4Z",
        "outputId": "2a6dd2c4-46ad-422f-9783-98270e86f46c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ed3d02f2-1fc3-4e70-aafe-86089cb2146a\", \"summaries_comparison.csv\", 1198086)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate NLP Scores"
      ],
      "metadata": {
        "id": "J_QfrXjS8L5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score nltk\n",
        "\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def calculate_bleu_score(references, hypotheses):\n",
        "    bleu_scores = [nltk.translate.bleu_score.sentence_bleu([ref.split()], hyp.split(), weights=(0.5, 0.5)) for ref, hyp in zip(references, hypotheses)]\n",
        "    return sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "def calculate_rouge_scores(references, hypotheses):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = [scorer.score(ref, hyp) for ref, hyp in zip(references, hypotheses)]\n",
        "    avg_scores = {key: sum([score[key].fmeasure for score in scores]) / len(scores) for key in ['rouge1', 'rouge2', 'rougeL']}\n",
        "    return avg_scores\n",
        "\n",
        "trained_bleu = calculate_bleu_score(target_summaries, trained_generated_summaries)\n",
        "baseline_bleu = calculate_bleu_score(target_summaries, baseline_generated_summaries)\n",
        "\n",
        "trained_rouge = calculate_rouge_scores(target_summaries, trained_generated_summaries)\n",
        "baseline_rouge = calculate_rouge_scores(target_summaries, baseline_generated_summaries)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odgxyXoJOcn-",
        "outputId": "8b207637-5f58-41c0-b6d8-a631071d8011"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print table"
      ],
      "metadata": {
        "id": "C4731RgG8TcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Metric': ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
        "    'Trained Model': [trained_bleu, trained_rouge_1, trained_rouge_2, trained_rouge_L],\n",
        "    'Baseline Model': [baseline_bleu, baseline_rouge_1, baseline_rouge_2, baseline_rouge_L]\n",
        "})\n",
        "\n",
        "# Display the table\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BV3Q1BEQDeu",
        "outputId": "fd234340-f5cb-4b05-822d-2324d359fd3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Metric  Trained Model  Baseline Model\n",
            "0     BLEU       0.228966        0.018471\n",
            "1  ROUGE-1       0.416104        0.024515\n",
            "2  ROUGE-2       0.251672        0.011014\n",
            "3  ROUGE-L       0.268903        0.017273\n"
          ]
        }
      ]
    }
  ]
}